## 線性迴歸與損失函數（Day 2）

**日期**：2025-11-05  
**來源**：Google Developers - Machine Learning Crash Course  
**關聯主題**：迴歸、損失函數、梯度下降、模型訓練

---

### 📘 學習摘要

#### 📐 線性迴歸基本概念

- 線性迴歸用於預測「連續值」，例如房價、降雨量、行駛時間。找出特徵與標籤之間線性關係的一種統計方法。
- 基本模型公式：

  ```
  y` = b + w1 * x1
  ```
  - **y`（預測標籤）**
  - **w（權重 weight）**：決定特徵的影響程度  
  - **b（偏差 bias）**：整體的基準偏移量  
  - **x（輸入特徵）**
- 訓練過程是反覆調整 w、b來找到最佳模型，資料本身不會更新。
- 多特徵延伸：模型變為
  ```
  y` = b + w1 * x1+ w2 * x2+ ...
  ```

---

#### 📉 損失函數（Loss Function）

損失函數用來衡量模型預測與真實答案的距離，目標是在訓練中把損失降到最低。

計算方式：
損失只關心距離大小而非方向，常用兩種方法去掉符號:取絕對值或取平方

常見損失：

1. **L2 損失 / MSE（Mean Squared Error）**
   - 計算「平方差的平均」
   - 對大誤差非常敏感（平方會放大差距），讓模型更重視極端點。

2. **L1 損失 / MAE（Mean Absolute Error）**
   - 計算「絕對值差的平均」
   - 對離群值較不敏感

MSE 常用於線性迴歸，因為其數學特性讓模型更容易優化。

[參數練習 (手動調整權重與偏差) ](https://developers.google.com/machine-learning/crash-course/linear-regression/parameters-exercise?hl=zh-tw#:~:text=dataset%2C%20with%20the,What%20weight%20and%20bias)

---

#### 🔄 梯度下降（Gradient Descent）

- 透過迭代計算損失相對於權重和偏差的導數(梯度)，沿著「負梯度方向」小步更新參數
以減少損失
- 用於更新模型參數，使損失逐步降低。
- 目標像是在山谷中找到最低點。
- 每次更新的公式大意為：

  ```
  w(new) = w(old) - α * (損失的梯度)
  b(new) = b(old) - α * (損失的梯度)
  ```

- **α（learning rate）學習率**：
  - 太大 → 無法收斂、震盪  
  - 太小 → 收斂很慢  

- 正確觀念：梯度下降只用來尋找最佳權重和偏差，它不會改變資料集，也不會決定用哪種損失函數

---

### 💡 前端應用

- 了解線性迴歸後，可以在前端更準確地判斷資料趨勢（例如金流趨勢、使用者行為預測）。
- 學習損失函數與梯度下降後，能更好理解後端或 ML 模型 API 的運作方式，例如：
  - 上線後為何模型需要重新訓練  
  - 為何模型在不同資料分布下會表現變差  
- 能協助產品端解釋「預測值是怎麼來的」並與後端合作排查模型異常。

---

### 🔗 延伸學習

- 什麼是 MSE 與 MAE？何時使用哪一種？  
- 線性迴歸的前提假設（線性關係、誤差常態分布）  
- 梯度下降的變形模式（SGD、Mini-batch、Momentum、Adam）  
- [Google ML Crash Course 線性迴歸單元](https://developers.google.com/machine-learning/crash-course/linear-regression)  
