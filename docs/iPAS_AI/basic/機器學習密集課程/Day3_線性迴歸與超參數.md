## 線性迴歸與超參數（Day 3）

**日期**：2025-11-06  
**來源**：Google Developers - Machine Learning Crash Course  
**關聯主題**：Learning Rate、Batch Size、Epoch、模型訓練、超參數調整

---

### 📘 學習摘要

#### 🔧 超參數（Hyperparameters）是什麼？

- 超參數是「由工程師設定、模型訓練過程不會自動學習」的參數。
- 常見例子：
  - **learning rate（學習率）**
  - **batch size（批次大小）**
  - **epochs（訓練週期數）**
- 超參數對訓練的穩定性與速度影響巨大。

> 參數：模型本身的變數(weights、bias)，在訓練過程中由演算法計算得出

---

#### ⚡ learning rate（學習率）

- 控制模型每次(梯度下降)更新時，參數的「步伐大小」。
- **太大**：
  - 損失在最小值附近上下震盪、無法收斂  
- **太小**：
  - 收斂速度極慢  
- 理想情況是找到「能穩定下降又不震盪」的適中 learning rate。

> 1. 與梯度的關係:更新量 = 梯度 × 學習率;例:梯度大小 2.5、學習率 0.01，參數改變 0.025 
> 2. 從損失曲線判斷:
>>   - 理想 LR:前幾十次迭代損失快速下降，之後趨於平緩並收斂。
>> -  LR 太小:每次只小幅改善，收斂需要很多迭代。
>>  -  LR 太大:曲線劇烈震盪或後期上升、無法收斂。
>3. 易錯點:把 LR 加倍可能反而變慢(因為太大導致「亂跳」)，最佳 LR 依資料與算力而定。
> 

---

#### 📦 batch size（批次大小）

- 一次讓模型看到的訓練資料數量。
- **小 batch（例如 32、64）：**
  - 更隨機，有助跳出局部最佳解  
  - 訓練較慢  
- **大 batch（例如 512、1024）：**
  - 平滑穩定，但容易陷入局部最佳解  
  - 訓練速度較快

> 1. 兩種常見做法:
>> - SGD(批次=1):更新很頻繁、噪聲大(損失曲線上下抖動)，但能快速探索解空間。
>>> SGD為 隨機梯度下降
>> - Mini-batch SGD:在 1 與全量之間折衷;隨機抽一小批樣本、平均梯度後再更新，抖動較
小、更穩定。
> 2. 取捨:小批次較像 SGD(速度快、噪聲大);大批次較像全量(穩定、但可能卡平臺);實務上視資料
與硬體調整。
> 3. 觀念補充:適度噪聲不一定是壞事，在之後的單元你會看到噪聲有助 泛化(避免只記住訓練集)。 
---

#### 🔁 epochs（訓練週期數）

- 1 epoch = 模型完整看過訓練資料一次。
- epoch 越高 ≠ 越好，太多會**過擬合（overfitting）**。
- 例如 1,000 筆資料、mini-batch=100，完成 1 個 epoch
需要 10 次迭代

> 1. 特性:通常需要多個 epoch 才能收斂;epoch 數需實驗決定，更多 epoch 往往能更好，但也更花時
間。
> 2. 更新次數換算(觀念題常考):
>> - Full-batch:每個 epoch 更新 1 次;20 個 epoch 就更新 20 次。
>> - SGD:每看 1 筆就更新 1 次;1,000 筆 × 20 個 epoch = 20,000 次更新。
>> - Mini-batch(例:100):每批更新 1 次;1,000 筆 × 20 epoch → 200 次更新。

---

#### 🔍 超參數彼此之間的關係

- 三者需互相平衡：
  - **學習率太大 + 大 batch** → 容易震盪  
  - **學習率太小 + 小 batch** → 訓練極慢  
  - **epoch 太多** → 過擬合  
- 因此訓練模型常需要反覆調整組合、試出最佳設定。

---

### 💡 前端應用

身為前端工程師，理解超參數有助於：

- 與資料科學家、後端討論模型調整時，能更精準理解問題來源（例如：模型為何突然表現變差）。
- 理解 edge case 產生原因，能在前端 UI 或資料檢查層做更完善的 validation。
- 建立前端小型 ML demo（如模式分類、簡易預測）時，可自行微調 hyperparameters。
- 在串接模型 API 時，能理解「模型重新訓練」的必要性與成本。

---

### 🔗 延伸學習

- 學習率調整策略（Learning Rate Decay、Cyclic LR、Warmup）
- 批次大小如何影響 GPU 計算效率  
- Early Stopping：避免 epoch 過多造成過擬合  
- [Google ML Crash Course 超參數章節](https://developers.google.com/machine-learning/crash-course/first-steps-with-tensorflow/programming-exercises
)  